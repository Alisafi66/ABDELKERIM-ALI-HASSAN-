{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "korean-wednesday",
      "metadata": {
        "id": "korean-wednesday"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "You should build an end-to-end machine learning pipeline using a recurrent neural network model. In particular, you should do the following:\n",
        "- Load the `jena climate` dataset using [Pandas](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). You can find this dataset in the [keras repository](https://keras.io/examples/timeseries/timeseries_weather_forecasting/).\n",
        "- Split the dataset into training, validation, and test sets. Note that you cannot split time series using [Scikit-Learn](https://keras.io/examples/timeseries/timeseries_weather_forecasting/).\n",
        "- Build an end-to-end machine learning pipeline, including a [recurrent neural network](https://keras.io/examples/timeseries/timeseries_weather_forecasting/) model.\n",
        "- Optimize your pipeline by validating your design decisions.\n",
        "- Test the best pipeline on the test set and report various [evaluation metrics](https://scikit-learn.org/0.15/modules/model_evaluation.html).  \n",
        "- Check the documentation to identify the most important hyperparameters, attributes, and methods of the model. Use them in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading, Sampling, and Target Creation"
      ],
      "metadata": {
        "id": "S9hHNF833UGI"
      },
      "id": "S9hHNF833UGI"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "infrared-copper",
      "metadata": {
        "id": "infrared-copper"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "uri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\n",
        "zip_path = keras.utils.get_file(origin=uri, fname=\"jena_climate_2009_2016.csv.zip\")\n",
        "\n",
        "csv_file_name = \"jena_climate_2009_2016.csv\"\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extract(csv_file_name)\n",
        "    df = pd.read_csv(csv_file_name)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{csv_file_name}' was not found after extraction.\")\n",
        "    df = None\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during file extraction or reading: {e}\")\n",
        "    df = None\n",
        "if df is not None:\n",
        "    df = df.iloc[5::6].reset_index(drop=True)\n",
        "\n",
        "    df['Date Time'] = pd.to_datetime(df['Date Time'], format='%d.%m.%Y %H:%M:%S')\n",
        "\n",
        "    df = df[df['Date Time'].dt.hour == 1].reset_index(drop=True)\n",
        "\n",
        "    feature_cols = ['T (degC)', 'p (mbar)', 'rh (%)', 'wv (m/s)', 'VPact (mbar)']\n",
        "    data = df[feature_cols].copy()\n",
        "\n",
        "    data['Target_T'] = data['T (degC)'].shift(-1)\n",
        "\n",
        "    data.dropna(inplace=True)\n",
        "\n",
        "    X = data[feature_cols].values\n",
        "    Y = data['Target_T'].values\n",
        "else:\n",
        "\n",
        "    print(\"DataFrame 'df' could not be loaded. Subsequent data processing steps have been skipped.\")"
      ],
      "metadata": {
        "id": "RlpvdnLC1oUp"
      },
      "id": "RlpvdnLC1oUp",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Splitting and Normalization"
      ],
      "metadata": {
        "id": "UtOd1bzN3kaq"
      },
      "id": "UtOd1bzN3kaq"
    },
    {
      "cell_type": "code",
      "source": [
        "train_split_pct = 0.7\n",
        "val_split_pct = 0.15\n",
        "test_split_pct = 0.15\n",
        "\n",
        "N = X.shape[0]\n",
        "train_end = int(train_split_pct * N)\n",
        "val_end = int((train_split_pct + val_split_pct) * N)\n",
        "\n",
        "X_train_raw, Y_train = X[:train_end], Y[:train_end]\n",
        "X_val_raw, Y_val = X[train_end:val_end], Y[train_end:val_end]\n",
        "X_test_raw, Y_test = X[val_end:], Y[val_end:]\n",
        "\n",
        "train_mean = X_train_raw.mean(axis=0)\n",
        "train_std = X_train_raw.std(axis=0)\n",
        "\n",
        "X_train = (X_train_raw - train_mean) / train_std\n",
        "X_val = (X_val_raw - train_mean) / train_std\n",
        "X_test = (X_test_raw - train_mean) / train_std"
      ],
      "metadata": {
        "id": "Vu-1icU73lK3"
      },
      "id": "Vu-1icU73lK3",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequence Matrix Creation"
      ],
      "metadata": {
        "id": "SOCGpoQp3rc9"
      },
      "id": "SOCGpoQp3rc9"
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 5\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "def create_time_series_dataset(features, targets, seq_len, batch_size):\n",
        "    feature_tensor = tf.constant(features, dtype=tf.float32)\n",
        "    target_tensor = tf.constant(targets, dtype=tf.float32)\n",
        "\n",
        "    data = keras.utils.timeseries_dataset_from_array(\n",
        "        data=feature_tensor,\n",
        "        targets=target_tensor[seq_len:],\n",
        "        sequence_length=seq_len,\n",
        "        sequence_stride=1,\n",
        "        sampling_rate=1,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    return data\n",
        "\n",
        "train_dataset = create_time_series_dataset(X_train, Y_train, SEQUENCE_LENGTH, BATCH_SIZE)\n",
        "val_dataset = create_time_series_dataset(X_val, Y_val, SEQUENCE_LENGTH, BATCH_SIZE)\n",
        "test_dataset = create_time_series_dataset(X_test, Y_test, SEQUENCE_LENGTH, BATCH_SIZE)\n",
        "\n",
        "for batch in train_dataset.take(1):\n",
        "    inputs, targets = batch\n",
        "    INPUT_SHAPE = (inputs.shape[1], inputs.shape[2])"
      ],
      "metadata": {
        "id": "c2N3Y6ca3sB5"
      },
      "id": "c2N3Y6ca3sB5",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Building, Training, and Evaluation"
      ],
      "metadata": {
        "id": "obPYANJo3ztE"
      },
      "id": "obPYANJo3ztE"
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 20\n",
        "LSTM_UNITS = 32\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.GRU(LSTM_UNITS, input_shape=INPUT_SHAPE),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=\"mse\"\n",
        ")\n",
        "\n",
        "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[es_callback],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\" Model Training Complete\")\n",
        "\n",
        "print(\"Evaluating model on the test set...\")\n",
        "test_loss = model.evaluate(test_dataset, verbose=0)\n",
        "print(f\"Test Mean Squared Error (MSE): {test_loss:.4f}\")\n",
        "\n",
        "sample_input, sample_target = next(iter(test_dataset))\n",
        "prediction = model.predict(sample_input, verbose=0)[0]\n",
        "actual = sample_target.numpy()[0]\n",
        "\n",
        "print(\"\\n Example Prediction\")\n",
        "print(f\"Predicted Next Day Temp (normalized): {prediction[0]:.4f}\")\n",
        "print(f\"Actual Next Day Temp (normalized):   {actual:.4f}\")\n",
        "\n",
        "print(\"\\n Hyperparameter & Method Check \")\n",
        "print(f\"Hyperparameter (GRU Units): {LSTM_UNITS}\")\n",
        "print(f\"Hyperparameter (Sequence Length): {SEQUENCE_LENGTH}\")\n",
        "print(f\"Method Used: model.fit() (Training)\")\n",
        "print(f\"Method Used: model.evaluate() (Testing)\")\n",
        "print(f\"Attribute Check (Total Parameters): {model.count_params()}\")"
      ],
      "metadata": {
        "id": "4BfBHkI930Sj",
        "outputId": "02b94c23-eb77-4a89-91e7-e68f53bed018",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4BfBHkI930Sj",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model Training Complete\n",
            "Evaluating model on the test set...\n",
            "Test Mean Squared Error (MSE): 16.6793\n",
            "\n",
            " Example Prediction\n",
            "Predicted Next Day Temp (normalized): 7.0987\n",
            "Actual Next Day Temp (normalized):   10.0000\n",
            "\n",
            " Hyperparameter & Method Check \n",
            "Hyperparameter (GRU Units): 32\n",
            "Hyperparameter (Sequence Length): 5\n",
            "Method Used: model.fit() (Training)\n",
            "Method Used: model.evaluate() (Testing)\n",
            "Attribute Check (Total Parameters): 3777\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}